---
---




@article{zhao2022KSetGNN,
  title={A Practical, Progressively-Expressive GNN},
  author={Zhao, Lingxiao and Shah, Neil and Akoglu, Leman},
  journal={NeurIPS},
  year={2022},
  abstract={Message passing neural networks (MPNNs) have become a dominant flavor of graph neural networks (GNNs) in recent years. Yet, MPNNs come with notable limitations; namely, they are at most as powerful as the 1-dimensional Weisfeiler-Leman (1-WL) test in distinguishing graphs in a graph isomorphism testing frame-work. To this end, researchers have drawn inspiration from the k-WL hierarchy to develop more expressive GNNs. However, current k-WL-equivalent GNNs are not practical for even small values of k, as k-WL becomes combinatorially more complex as k grows. At the same time, several works have found great empirical success in graph learning tasks without highly expressive models, implying that chasing expressiveness with a “coarse-grained ruler” of expressivity like k-WL is often unneeded in practical tasks. To truly understand the expressiveness-complexity tradeoff, one desires a more “fine-grained ruler,” which can more gradually increase expressiveness. Our work puts forth such a proposal: Namely, we first propose the (k, c)(≤)-SETWL hierarchy with greatly reduced complexity from k-WL, achieved by moving from k-tuples of nodes to sets with ≤k nodes defined over ≤c connected components in the induced original graph. We show favorable theoretical results for this model in relation to k-WL, and concretize it via (k, c)(≤)-SETGNN, which is as expressive as (k, c)(≤)-SETWL. Our model is practical and progressively-expressive, increasing in power with k and c. We demonstrate effectiveness on several benchmark datasets, achieving several state-of-the-art results with runtime and memory usage applicable to practical graphs.},
}


@article{ding2022HyperEnsemble,
  title={Hyperparameter Sensitivity in Deep Outlier Detection: Analysis and a Scalable Hyper-Ensemble Solution},
  author={Ding, Xueying and Zhao, Lingxiao and Akoglu, Leman},
  journal={NeurIPS},
  year={2022},
  abstract={Outlier detection (OD) literature exhibits numerous algorithms as it applies to diverse domains. However, given a new detection task, it is unclear how to choose an algorithm to use, nor how to set its hyperparameter(s) (HPs) in unsupervised settings. HP tuning is an ever-growing problem with the arrival of many new detectors based on deep learning. While they have appealing properties such as task- driven representation learning and end-to-end optimization, deep models come with a long list of HPs. Surprisingly, the issue of model selection in the outlier mining literature has been "the elephant in the room"; a significant factor in unlocking the utmost potential of deep methods, yet little said or done to systematically tackle the issue. In the first part of this paper, we conduct the first large-scale analysis on the HP sensitivity of deep OD methods, and through more than 35,000 trained models, quantitatively demonstrate that model selection is inevitable. Next, we design a HP-robust and scalable deep hyper-ensemble model called ROBOD that assembles models with varying HP configurations, bypassing the choice paralysis. Importantly, we introduce novel strategies to speed up ensemble training, such as parameter sharing, batch/simultaneous training, and data subsampling, that allow us to train fewer models with fewer parameters. Extensive experiments on both image and tabular datasets show that ROBOD achieves and retains robust, state-of-the-art detection performance as compared to its modern counterparts, while taking only 2-10% of the time by the naive hyper-ensemble with independent training.},
  extpdf={https://arxiv.org/pdf/2206.07647.pdf},
}



@article{2022GLAM,
  title={Graph-level Anomaly Detection with Unsupervised GNNs},
  author={Zhao, Lingxiao and Sawlani, Saurabh and Srinivasan, Arvind and Akoglu, Leman},
  journal={ICDM (Short Paper)},
  year={2022},
  abstract={Graph-based anomaly detection ﬁnds numerous applications in the real-world. Thus, there exists extensive literature on the topic that has recently shifted toward deep detection models due to advances in deep learning and graph neural networks (GNNs). A vast majority of prior work focuses on detecting node/edge/subgraph anomalies within a single graph, with much less work on graph-level anomaly detection in a graph database. This work aims to ﬁll two gaps in the literature: We (1) design GLAM, an end-to-end graph-level anomaly detection model based on GNNs, and (2) focus on unsupervised model selection, which is notoriously hard due to lack of any labels, yet especially critical for deep NN based models with a long list of hyperparameters. Further, we propose a new pooling strategy for graph-level embedding, called MMD-pooling, that is geared toward detecting distribution anomalies which has not been considered before. Through extensive experiments on 15 real-world datasets, we show that (i) GLAM outperforms node-level and two-stage (i.e. not end-to-end) baselines, and (ii) model selection picks a signiﬁcantly more effective model than expectation (i.e. average) –without using any labels– among candidates with otherwise large variation in performance.},
}

@article{2022DOS_Journal,
  title={Density of States for Fast Embedding Node-Attributed Graphs},
  author={Zhao, Lingxiao and Sawlani, Saurabh and Akoglu, Leman},
  journal={Knowledge and Information Systems (KAIS) (Journal)},
  year={2022},
}


@article{Lim2022SignNet,
  title={Sign and Basis Invariant Networks for Spectral Graph Representation Learning},
  author={Lim*, Derek and Robinson*, Joshua and Zhao, Lingxiao and Smidt, Tess and Sra, Suvrit and Maron, Haggai and Jegelka, Stefanie},
  journal={Under review},
  year={2022},
  abstract={Many machine learning tasks involve processing eigenvectors derived from data. Especially valuable are Laplacian eigenvectors, which capture useful structural information about graphs and other geometric objects. However, ambiguities arise when computing eigenvectors: for each eigenvector v, the sign flipped -v is also an eigenvector. More generally, higher dimensional eigenspaces contain infinitely many choices of basis eigenvectors. These ambiguities make it a challenge to process eigenvectors and eigenspaces in a consistent way. In this work we introduce SignNet and BasisNet -- new neural architectures that are invariant to all requisite symmetries and hence process collections of eigenspaces in a principled manner. Our networks are universal, i.e., they can approximate any continuous function of eigenvectors with the proper invariances. They are also theoretically strong for graph representation learning -- they can approximate any spectral graph convolution, can compute spectral invariants that go beyond message passing neural networks, and can provably simulate previously proposed graph positional encodings. Experiments show the strength of our networks for learning spectral graph filters and learning graph positional encodings.},
  extpdf={https://arxiv.org/pdf/2202.13013.pdf},
}

@article{zhao2021GNNAsKernel,
  title={From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness},
  author={Zhao, Lingxiao and Jin, Wei and Akoglu, Leman and Shah, Neil},
  journal={ICLR},
  year={2022},
  abstract={Message Passing Neural Networks (MPNNs) are a common type of Graph Neural
            Network (GNN), in which each node’s representation is computed recursively by
            aggregating representations (“messages”) from its immediate neighbors akin to a
            star-shaped pattern. MPNNs are appealing for being efficient and scalable, 
            however their expressiveness is upper-bounded by the 1st-order Weisfeiler-Lehman
            isomorphism test (1-WL). In response, prior works propose highly expressive
            models at the cost of scalability and sometimes generalization performance. Our
            work stands between these two regimes: we introduce a general framework to
            uplift any MPNN to be more expressive, with limited scalability overhead and
            greatly improved practical performance. We achieve this by extending local aggregation in 
            MPNNs from star patterns to general subgraph patterns (e.g., k-egonets):
            in our framework, each node representation is computed as the encoding of a surrounding induced subgraph 
            rather than encoding of immediate neighbors only (i.e.
            a star). We choose the subgraph encoder to be a GNN (mainly MPNNs, considering scalability) to 
            design a general framework that serves as a wrapper to uplift any GNN. We call our proposed method GNN-AK (GNN As Kernel), as the
            framework resembles a convolutional neural network by replacing the kernel with
            GNNs. Theoretically, we show that our framework is strictly more powerful than
            1&2-WL, and is not less powerful than 3-WL. We also design subgraph sampling
            strategies which greatly reduce memory footprint and improve speed while maintaining performance. 
            Our method sets new state-of-the-art performance by large
            margins for several well-known graph ML tasks; specifically, 0.08 MAE on ZINC,
            74.79% and 86.887% accuracy on CIFAR10 and PATTERN respectively.},
  extpdf={https://arxiv.org/pdf/2110.03753.pdf},
  code={https://github.com/GNNAsKernel/GNNAsKernel},
  slides={22-ICLR-GNN-AK.pdf},
}

@article{wei2021condensation,
  title={Graph Condensation for Graph Neural Networks},
  author={Jin, Wei and Zhao, Lingxiao and Zhang, Shichang and Liu, Yozen and Tang, Jiliang and Shah, Neil},
  journal={ICLR},
  year={2022},
  abstract={Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. 
  To alleviate the concerns, we propose and study the problem of graph condensation for graph neural networks (GNNs). 
  Specifically, we aim to condense the large, original graph into a small, synthetic and highly-informative graph, such that GNNs
  trained on the small graph and large graph have comparable performance. We approach the condensation problem by imitating the GNN training trajectory on the
  original graph through the optimization of a gradient matching loss and design a strategy to condense node futures and structural information simultaneously. 
  Extensive experiments have demonstrated the effectiveness of the proposed framework in condensing different graph datasets into informative smaller graphs. In
  particular, we are able to approximate the original test accuracy by 95.3% on Reddit, 99.8% on Flickr and 99.0% on Citeseer, while reducing their graph size by
  more than 99.9%, and the condensed graphs can be used to train various GNN
  architectures.},
  extpdf={https://arxiv.org/pdf/2110.07580.pdf},
  code={https://github.com/ChandlerBang/GCond},
}

@article{Sawlani2021DOS,
  title={Fast Attributed Graph Embedding via Density of States},
  author={Sawlani, Saurabh and Zhao, Lingxiao and Akoglu, Leman},
  journal={IEEE ICDM 2021},
  year={2021},
  abstract={Given a node-attributed graph, how can we efficiently represent it with few numerical features that expressively
            reflect its topology and attribute information? We propose A-DOGE, for Attributed DOS-based Graph Embedding, based on
            density of states (DOS, a.k.a. spectral density) to tackle this
            problem. A-DOGE is designed to fulfill a long desiderata of
            desirable characteristics. Most notably, it capitalizes on efficient
            approximation algorithms for DOS, that we extend to blend in
            node labels and attributes for the first time, making it fast and
            scalable for large attributed graphs and graph databases. Being
            based on the entire eigenspectrum of a graph, A-DOGE can
            capture structural and attribute properties at multiple (“glocal”)
            scales. Moreover, it is unsupervised (i.e. agnostic to any specific
            objective) and lends itself to various interpretations, which makes
            it is suitable for exploratory graph mining tasks. Finally, it pro-
            cesses each graph independent of others, making it amenable for
            streaming settings as well as parallelization. Through extensive
            experiments, we show the efficacy and efficiency of A-DOGE on
            exploratory graph analysis and graph classification tasks, where
            it significantly outperforms unsupervised baselines and achieves
            competitive performance with modern supervised GNNs, while
            achieving the best trade-off between accuracy and runtime.},
  extpdf={https://arxiv.org/pdf/2110.05228.pdf},
  code={https://github.com/sawlani/A-DOGE}
}


@article{zhao2021gpca,
  title={Connecting Graph Convolutional Network and Graph-Regularized PCA (Extended)},
  author={Zhao, Lingxiao and Akoglu, Leman},
  journal={Under review},
  year={2021},
  abstract={Graph convolution operator of the GCN model is originally motivated from a localized ﬁrst-order approximation of spectral graph convolutions. 
            This work stands on a different view; establishing a mathematical connection between graph convolution and graph-regularized PCA (GPCA). 
            Based on this connection, GCN architecture, shaped by stacking graph convolution layers, shares a close relationship with stacking GPCA. 
            We empirically demonstrate that the unsupervised embeddings by GPCA paired with a 1- or 2-layer MLP achieves similar or even better performance 
            than GCN on semi-supervised node classiﬁcation tasks across ﬁve datasets including Open Graph Benchmark 1. 
            This suggests that the prowess of GCN is driven by graph based regularization. 
            In addition, we extend GPCA to the (semi-)supervised setting and show that it is equivalent to GPCA on a graph extended with “ghost” edges 
            between nodes of the same label. Finally, we capitalize on the discovered relationship to design an effective initialization strategy based on stacking GPCA, 
            enabling GCN to converge faster and achieve robust performance at large number of layers. Notably, the proposed initialization is general-purpose and applies to other GNNs.},
  extpdf={https://arxiv.org/pdf/2006.12294.pdf},
  code={https://github.com/LingxiaoShawn/GPCANet}
}

@article{zhao2021issue,
  title={On Using Classification Datasets to Evaluate Graph Outlier Detection: Peculiar Observations and New Insights},
  author={Zhao, Lingxiao and Akoglu, Leman},
  journal={Big Data Journal, Special Issue on Evaluation and Experimental Design in Data Mining and Machine Learning, Aug. 2021},
  year={2021},
  abstract={It is common practice of the outlier mining community to repurpose classification datasets toward evaluating
            various detection models. To that end, often a binary classification dataset is used, where samples from
            one of the classes is designated as the ‘inlier’ samples, and the other class is substantially down-sampled
            to create the (ground-truth) ‘outlier’ samples. Graph-level outlier detection (GLOD) is rarely studied but
            has many potentially influential real-world applications. In this study, we identify an intriguing issue with
            repurposing graph classification datasets for GLOD. We find that ROC-AUC performance of the models
            changes significantly (“flips” from high to very low, even worse than random) depending on which class is
            down-sampled. Interestingly, ROC-AUCs on these two variants approximately sum to 1 and their performance
            gap is amplified with increasing propagations for a certain family of propagation based outlier detection
            models. We carefully study the graph embedding space produced by propagation based models and find
            two driving factors: (1) disparity between within-class densities which is amplified by propagation, and (2)
            overlapping support (mixing of embeddings) across classes. We also study other graph embedding methods and
            downstream outlier detectors, and find that the intriguing “performance flip” issue still widely exists but which
            version of the downsample achieves higher performance may vary. Thoughtful analysis over comprehensive
            results further deeper our understanding of the established issue.},
  extpdf={https://arxiv.org/pdf/2012.12931.pdf},
  code={https://github.com/LingxiaoShawn/GLOD-Issues}
}

@article{zhao2019pairnorm,
  title={PairNorm: Tackling Oversmoothing in GNNs},
  author={Zhao, Lingxiao and Akoglu, Leman},
  journal={ICLR 2020, Addis Ababa, Ethiopia},
  year={2020},
  abstract={The performance of graph neural nets (GNNs) is known to gradually decrease
            with increasing number of layers. This decay is partly attributed to oversmoothing, where repeated graph convolutions eventually make node embeddings indistinguishable. We take a closer look at two different interpretations, aiming to
            quantify oversmoothing. Our main contribution is PAIRNORM, a novel normalization layer that is based on a careful analysis of the graph convolution operator,
            which prevents all node embeddings from becoming too similar. What is more,
            PAIRNORM is fast, easy to implement without any change to network architecture
            nor any additional parameters, and is broadly applicable to any GNN. Experiments
            on real-world graphs demonstrate that PAIRNORM makes deeper GCN, GAT, and
            SGC models more robust against oversmoothing, and significantly boosts performance for a new problem setting that benefits from deeper GNNs.},
  extpdf={https://openreview.net/pdf?id=rkecl1rtwB},
  code={https://github.com/LingxiaoShawn/PairNorm},
  slides={20-ICLR-PairNorm.pdf},
  video={https://iclr.cc/virtual_2020/poster_rkecl1rtwB.html}
}

@inproceedings{wu2018quest,
  title={A Quest for Structure: Jointly Learning the Graph Structure and Semi-Supervised Classification},
  author={Wu*, Xuan and Zhao*, Lingxiao and Akoglu, Leman},
  booktitle={Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  pages={87--96},
  year={2018},
  organization={ACM},
  abstract={How should one construct a graph from the input point-cloud
            data for graph-based SSL? In this work we introduce a new, parallel graph learning framework (called PG-learn) for the graph
            construction step of SSL. Our solution has two main ingredients:
            (1) a gradient-based optimization of the edge weights (more specifically, different kernel bandwidths in each dimension) based on a
            validation loss function, and (2) a parallel hyperparameter search algorithm with an adaptive resource allocation scheme. In essence, (1)
            allows us to search around a (random) initial hyperparameter configuration for a better one with lower validation loss. Since the search
            space of hyperparameters is huge for high-dimensional problems,
            (2) empowers our gradient-based search to go through as many
            different initial configurations as possible, where runs for relatively
            unpromising starting configurations are terminated early to allocate the time for others. As such, PG-learn is a carefully-designed
            hybrid of random and adaptive search. Through experiments on
            multi-class classification problems, we show that PG-learn significantly outperforms a variety of existing graph construction
            schemes in accuracy (per fixed time budget for hyperparameter
            tuning), and scales more effectively to high dimensional problems.},
  extpdf={https://arxiv.org/pdf/1909.12385.pdf},
  code={https://github.com/LingxiaoShawn/PG-Learn},
  slides={18-cikm-graph_SSL.pdf}
}

@article{zhao2020gpca,
  title={Connecting Graph Convolutional Network and Graph-Regularized PCA},
  author={Zhao, Lingxiao and Akoglu, Leman},
  journal={ICML 2020 GRLP Workshop},
  year={2020},
  abstract={Graph convolution operator of the GCN model is originally motivated from a localized first-order approximation of spectral graph convolutions.This work stands on a different view; establishing a connection between graph convolution and graph-regularized PCA. Based on this connection, GCN architecture, shaped by stacking graph convolution layers, shares a close relationship with stacking graph-regularized PCA (GPCA). We empirically demonstrate that the unsupervised embeddings by GPCA paired with a logistic regression classifier achieves similar performance to GCN on semi-supervised node classification tasks. Further, we capitalize on the discovered relationship to design an effective initialization strategy for GCN based on stacking GPCA.},
  extpdf={https://arxiv.org/pdf/2006.12294.pdf}
}

@article{zhu2020homophily,
  title={Generalizing Graph Neural Networks Beyond Homophily},
  author={Zhu, Jiong and Yan, Yujun and Zhao, Lingxiao and Heimann, Mark and Akoglu, Leman and   Koutra, Danai},
  journal={arXiv preprint arXiv:2006.11468},
  year={2020},
  abstract={We investigate the representation power of graph neural networks in the semi-supervised node classification task under heterophily or low homophily, i.e., in networks where connected nodes may have different class labels and dissimilar features. Most existing GNNs fail to generalize to this setting, and are even outperformed by models that ignore the graph structure (e.g., multilayer perceptrons). Motivated by this limitation, we identify a set of key designs -- ego- and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations -- that boost learning from the graph structure under heterophily, and combine them into a new graph convolutional neural network, H2GCN. Going beyond the traditional benchmarks with strong homophily, our empirical analysis on synthetic and real networks shows that, thanks to the identified designs, H2GCN has consistently strong performance across the full spectrum of low-to-high homophily, unlike competitive prior models without them.},
  extpdf={https://arxiv.org/abs/2006.11468.pdf}
}

@article{chen2020graph,
  title={Graph Unrolling Networks: Interpretable Neural Networks for Graph Signal Denoising},
  author={Chen, Siheng and Eldar, Yonina C and Zhao, Lingxiao},
  journal={arXiv preprint arXiv:2006.01301},
  year={2020},
  extpdf={https://arxiv.org/pdf/2006.01301.pdf},
  abstract={We propose an interpretable graph neural network framework to denoise single or multiple noisy graph signals. The proposed graph unrolling networks expand algorithm unrolling to the graph domain and provide an interpretation of the architecture design from a signal processing perspective. We unroll an iterative denoising algorithm by mapping each iteration into a single network layer where the feed-forward process is equivalent to iteratively denoising graph signals. We train the graph unrolling networks through unsupervised learning, where the input noisy graph signals are used to supervise the networks. By leveraging the learning ability of neural networks, we adaptively capture appropriate priors from input noisy graph signals, instead of manually choosing signal priors. A core component of graph unrolling networks is the edge-weight-sharing graph convolution operation, which parameterizes each edge weight by a trainable kernel function where the trainable parameters are shared by all the edges. The proposed convolution is permutation-equivariant and can flexibly adjust the edge weights to various graph signals. We then consider two special cases of this class of networks, graph unrolling sparse coding (GUSC) and graph unrolling trend filtering (GUTF), by unrolling sparse coding and trend filtering, respectively. To validate the proposed methods, we conduct extensive experiments on both real-world datasets and simulated datasets, and demonstrate that our methods have smaller denoising errors than conventional denoising algorithms and state-of-the-art graph neural networks. For denoising a single smooth graph signal, the normalized mean square error of the proposed networks is around 40% and 60% lower than that of graph Laplacian denoising and graph wavelets, respectively.}
}





