---
---

@article{zhao2021gpca,
  title={Connecting Graph Convolutional Network and Graph-Regularized PCA (Extended)},
  author={Zhao, Lingxiao and Akoglu, Leman},
  journal={Under review at ICML 2021},
  year={2021},
  abstract={Graph convolution operator of the GCN model is originally motivated from a localized ﬁrst-order approximation of spectral graph convolutions. This work stands on a different view; establishing a mathematical connection between graph convolution and graph-regularized PCA (GPCA). Based on this connection, GCN architecture, shaped by stacking graph convolution layers, shares a close relationship with stacking GPCA. We empirically demonstrate that the unsupervised embeddings by GPCA paired with a 1- or 2-layer MLP achieves similar or even better performance than GCN on semi-supervised node classiﬁcation tasks across ﬁve datasets including Open Graph Benchmark 1. This suggests that the prowess of GCN is driven by graph based regularization. In addition, we extend GPCA to the (semi-)supervised setting and show that it is equivalent to GPCA on a graph extended with “ghost” edges between nodes of the same label. Finally, we capitalize on the discovered relationship to design an effective initialization strategy based on stacking GPCA, enabling GCN to converge faster and achieve robust performance at large number of layers. Notably, the proposed initialization is general-purpose and applies to other GNNs.},
  extpdf={21-GPCA.pdf}
}

@article{zhao2021issue,
  title={Issues with Propagation Based Models for Graph-Level Outlier Detection},
  author={Zhao, Lingxiao and Akoglu, Leman},
  journal={Under review at ICML 2021},
  year={2021},
  abstract={Graph-Level Outlier Detection (GLOD) is the task of identifying unusual graphs within a graph database, which received little attention compared to node-level detection in a single graph. As propagation based graph embedding by GNNs and graph kernels achieved promising results on another graph-level task, i.e. graph classiﬁcation, we study applying those models to tackle GLOD. Instead of developing new models, this paper identiﬁes and delves into a fundamental and intriguing issue with applying propagation based models to GLOD, with evaluation conducted on repurposed binary graph classiﬁcation datasets where one class is down-sampled as outlier. We ﬁnd that ROC-AUC performance of the models changes signiﬁcantly (“ﬂips” from high to low) depending on which class is down-sampled. Interestingly, ROC-AUCs on these two variants approximately sum to 1 and their performance gap is ampliﬁed with increasing propagations. We carefully study the graph embedding space produced by propagation based models and ﬁnd two driving factors: (1) disparity between within-class densities which is ampliﬁed by propagation, and (2) overlapping support (mixing of embeddings) across classes. Our study sheds light onto the effects of using graph propagation based models and classiﬁcation datasets for outlier detection for the ﬁrst time.},
  extpdf={21-PerformanceFlip.pdf}
}

@article{zhao2019pairnorm,
  title={PairNorm: Tackling Oversmoothing in GNNs},
  author={Zhao, Lingxiao and Akoglu, Leman},
  journal={ICLR 2020, Addis Ababa, Ethiopia},
  year={2020},
  abstract={The performance of graph neural nets (GNNs) is known to gradually decrease
            with increasing number of layers. This decay is partly attributed to oversmoothing, where repeated graph convolutions eventually make node embeddings indistinguishable. We take a closer look at two different interpretations, aiming to
            quantify oversmoothing. Our main contribution is PAIRNORM, a novel normalization layer that is based on a careful analysis of the graph convolution operator,
            which prevents all node embeddings from becoming too similar. What is more,
            PAIRNORM is fast, easy to implement without any change to network architecture
            nor any additional parameters, and is broadly applicable to any GNN. Experiments
            on real-world graphs demonstrate that PAIRNORM makes deeper GCN, GAT, and
            SGC models more robust against oversmoothing, and significantly boosts performance for a new problem setting that benefits from deeper GNNs.},
  extpdf={https://openreview.net/pdf?id=rkecl1rtwB},
  code={https://github.com/LingxiaoShawn/PairNorm},
  slides={20-ICLR-PairNorm.pdf},
  video={https://iclr.cc/virtual_2020/poster_rkecl1rtwB.html}
}

@inproceedings{wu2018quest,
  title={A Quest for Structure: Jointly Learning the Graph Structure and Semi-Supervised Classification},
  author={Wu*, Xuan and Zhao*, Lingxiao and Akoglu, Leman},
  booktitle={Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
  pages={87--96},
  year={2018},
  organization={ACM},
  abstract={How should one construct a graph from the input point-cloud
            data for graph-based SSL? In this work we introduce a new, parallel graph learning framework (called PG-learn) for the graph
            construction step of SSL. Our solution has two main ingredients:
            (1) a gradient-based optimization of the edge weights (more specifically, different kernel bandwidths in each dimension) based on a
            validation loss function, and (2) a parallel hyperparameter search algorithm with an adaptive resource allocation scheme. In essence, (1)
            allows us to search around a (random) initial hyperparameter configuration for a better one with lower validation loss. Since the search
            space of hyperparameters is huge for high-dimensional problems,
            (2) empowers our gradient-based search to go through as many
            different initial configurations as possible, where runs for relatively
            unpromising starting configurations are terminated early to allocate the time for others. As such, PG-learn is a carefully-designed
            hybrid of random and adaptive search. Through experiments on
            multi-class classification problems, we show that PG-learn significantly outperforms a variety of existing graph construction
            schemes in accuracy (per fixed time budget for hyperparameter
            tuning), and scales more effectively to high dimensional problems.},
  extpdf={https://arxiv.org/pdf/1909.12385.pdf},
  code={https://github.com/LingxiaoShawn/PG-Learn},
  slides={18-cikm-graph_SSL.pdf}
}

@article{zhao2020gpca,
  title={Connecting Graph Convolutional Network and Graph-Regularized PCA},
  author={Zhao, Lingxiao and Akoglu, Leman},
  journal={ICML 2020 GRLP Workshop},
  year={2020},
  abstract={Graph convolution operator of the GCN model is originally motivated from a localized first-order approximation of spectral graph convolutions.This work stands on a different view; establishing a connection between graph convolution and graph-regularized PCA. Based on this connection, GCN architecture, shaped by stacking graph convolution layers, shares a close relationship with stacking graph-regularized PCA (GPCA). We empirically demonstrate that the unsupervised embeddings by GPCA paired with a logistic regression classifier achieves similar performance to GCN on semi-supervised node classification tasks. Further, we capitalize on the discovered relationship to design an effective initialization strategy for GCN based on stacking GPCA.},
  extpdf={https://arxiv.org/pdf/2006.12294.pdf}
}

@article{zhu2020homophily,
  title={Generalizing Graph Neural Networks Beyond Homophily},
  author={Zhu, Jiong and Yan, Yujun and Zhao, Lingxiao and Heimann, Mark and Akoglu, Leman and   Koutra, Danai},
  journal={arXiv preprint arXiv:2006.11468},
  year={2020},
  abstract={We investigate the representation power of graph neural networks in the semi-supervised node classification task under heterophily or low homophily, i.e., in networks where connected nodes may have different class labels and dissimilar features. Most existing GNNs fail to generalize to this setting, and are even outperformed by models that ignore the graph structure (e.g., multilayer perceptrons). Motivated by this limitation, we identify a set of key designs -- ego- and neighbor-embedding separation, higher-order neighborhoods, and combination of intermediate representations -- that boost learning from the graph structure under heterophily, and combine them into a new graph convolutional neural network, H2GCN. Going beyond the traditional benchmarks with strong homophily, our empirical analysis on synthetic and real networks shows that, thanks to the identified designs, H2GCN has consistently strong performance across the full spectrum of low-to-high homophily, unlike competitive prior models without them.},
  extpdf={https://arxiv.org/abs/2006.11468.pdf}
}

@article{chen2020graph,
  title={Graph Unrolling Networks: Interpretable Neural Networks for Graph Signal Denoising},
  author={Chen, Siheng and Eldar, Yonina C and Zhao, Lingxiao},
  journal={arXiv preprint arXiv:2006.01301},
  year={2020},
  extpdf={https://arxiv.org/pdf/2006.01301.pdf},
  abstract={We propose an interpretable graph neural network framework to denoise single or multiple noisy graph signals. The proposed graph unrolling networks expand algorithm unrolling to the graph domain and provide an interpretation of the architecture design from a signal processing perspective. We unroll an iterative denoising algorithm by mapping each iteration into a single network layer where the feed-forward process is equivalent to iteratively denoising graph signals. We train the graph unrolling networks through unsupervised learning, where the input noisy graph signals are used to supervise the networks. By leveraging the learning ability of neural networks, we adaptively capture appropriate priors from input noisy graph signals, instead of manually choosing signal priors. A core component of graph unrolling networks is the edge-weight-sharing graph convolution operation, which parameterizes each edge weight by a trainable kernel function where the trainable parameters are shared by all the edges. The proposed convolution is permutation-equivariant and can flexibly adjust the edge weights to various graph signals. We then consider two special cases of this class of networks, graph unrolling sparse coding (GUSC) and graph unrolling trend filtering (GUTF), by unrolling sparse coding and trend filtering, respectively. To validate the proposed methods, we conduct extensive experiments on both real-world datasets and simulated datasets, and demonstrate that our methods have smaller denoising errors than conventional denoising algorithms and state-of-the-art graph neural networks. For denoising a single smooth graph signal, the normalized mean square error of the proposed networks is around 40% and 60% lower than that of graph Laplacian denoising and graph wavelets, respectively.}
}





